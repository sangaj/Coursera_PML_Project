---
title: "Coursera Practical Machine Learning-Case study"
author: Bingjing GU
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement Â¨C a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset).

## Data
The training data for this project are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 


## Data Preparation
we first load those packages I would use in this case project.

```{r packages,echo=TRUE,results='hide', message=FALSE}
### Loading Packages
library(caret)
library(rattle)
library(randomForest)
library(rpart)
library(rpart.plot)
library(C50)
library(plyr)
```

Then we read the train set and test set. There are 19622 observations and 160 variables which means the first step is to reduce the variables since there are too many variables there and some of them are multicollinearity and useless on prediction power. The first step I would do is to drop those variables contain NA values. After that we remove those "near-zero-variance" predictors which  need to be identified and eliminated before modeling. At last, we remove those meaningless features for prediction, such as first several columns. Now there are 54 columns remaining, one is the outcome classe. others are the features. The test set need the same features as the train set.

```{r introduction}
### read data and preprocessing data into train, validation and test set.
train <- read.table("pml-training.csv",header=T,sep=",")
test <- read.table("pml-testing.csv",header=T,sep=",")
train <- train[, colSums(is.na(train)) == 0]
nzv_cols <- nearZeroVar(train) # drop useless col
if(length(nzv_cols) > 0) train <- train[, -nzv_cols]
train <- train[, -(1:5)]
idx <- match(names(train), names(test))
idx  <- idx [!is.na(idx )]
test <- test[,idx] 
set.seed(2017) # make analysis reproducible
intrain <- createDataPartition(train$classe, p = 0.8, list = FALSE)
train <- train[intrain, ]
valid <- train[-intrain, ]

```

Then we need to split the train set to train set and validation set. As a rule of thumb, 80% data would be in new train set and 20% data would be into validation set.

## Algorithm Models

### Classification Trees
In the begining, I use the classification trees to predict the outcomes and check the performance of it.

```{r classification1}
### Apply classification Trees
control <- trainControl(method = "cv", number = 10)
rpart <- train(classe ~ ., data = train, method = "rpart", trControl = control)
print(rpart) 
fancyRpartPlot(rpart$finalModel) # plot the results
predict.rpart <- predict(rpart, valid)
conf.rpart <- confusionMatrix(valid$classe, predict.rpart)
accuracy.rpart <- conf.rpart$overall[1]
```


we start a 10-fold cross validation to fit the classification trees and get the accuracy $`r accuracy.rpart`$ on the validation set. It's not a quite good results on test set. The performance on train set is also bad. These imply that it would not be overfitting and underfitting. The method itself is not the best choice. So we need another method to substitute it.

### Random Forests

```{r rf}
### Apply Random Forests
control <- trainControl(method = "cv", number = 10)
rf <- train(classe ~ ., data = train, method = "rf", trControl = control)
print(rf)
predict.rf <- predict(rf, valid)
conf.rf <- confusionMatrix(valid$classe, predict.rf)
accuracy.rf <- conf.rf$overall[1]
```

we still use a 10-fold cross validation to fit the random forest and get the accuracy $`r accuracy.rf`$ on the validation set. It's just get the perfect results from it. since it just fit on the data we already have. It could still be overfitting on the another independet test set. Thus ,we introduce another method to check the performance.(10 fold CV may take a long time to reach the results, we may set it to 5 to reduce the running time)


### Boosting trees
```{r boost}
### Apply Boosting trees
control <- trainControl(method = "cv", number = 10)
grid <- expand.grid(.model = "tree",.trials = c(1:100),.winnow = FALSE)
c5 <- train(classe ~ ., data = train,method = "C5.0",tuneGrid = grid,trControl = control)
predict.c5 <- predict(c5, valid)
conf.c5 <- confusionMatrix(valid$classe, predict.c5)
accuracy.c5 <- conf.c5$overall[1]
```


we use a 10-fold cross validation to fit the boosting and get the accuracy $`r accuracy.rf`$ on the validation set. It's also just get the perfect results from it. Boosting tree is based on weak learners which gives high bias and low variance. Boosting reduces error mainly by reducing bias. On the other hand, Random Forest uses completely grown decision trees which has low bias and high variance. It tackles the error reduction task by reducing variance. These two methods both gives perfect results. Let's see what whould be in the test set.(10 fold CV may take a long time to reach the results, we may set it to 5 to reduce the running time)

### Application in Test set

Random forests and Boosting trees are far more better than claasification trees. We implement these two complicated methods based on claasification trees. 
```{r test}
### test set prediction
predict(rf, test)
predict(c5, test)
```





These two methods also give totally same results on the test set.
